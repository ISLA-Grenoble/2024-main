---
title: "Training and generalization error"
author: "Pedro L. C. Rodrigues"
date: "07-February 2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
# fixing seed so to ensure reproducible results and figures
set.seed(123)
```

We will reproduce the results portrayed in figures from Section 3.3 of Cosma Shalizi's book "*Advanced Data Analysis from an elementary point of view*". You can get a copy of this book [[here](https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/)].

In this whole script, we will be considering a generative model that relates the observation $Y$ with a predictor $X$ as per
$$
Y = 7X^2 - 0.5X + \varepsilon~,
$$
where $\varepsilon \sim \mathcal{N}(0, \sigma^2)$ is a Gaussian additive noise.

### 1) Generate data and plot it

To generate data points following the model described above, we can write:
```{r}
# fix the number of samples in the training dataset
N = 20
# choose at random the values for the xi
x = rnorm(N)
# generate the random noise
e = rnorm(N)
# get the observations from our model corrupted by noise
y = 7 * x^2 - 0.5 * x + e
# create a training dataframe which will be used to estimate the models
df.train = data.frame(y, x)
```

We can plot the data points and overlay them with the actual function that relates the variables with:
```{r}
# plot the data
plot(x, y)
# overlay the equation of the true relation between Y and X
curve(7 * x^2 - 0.5 * x, col = "grey", add = TRUE)
```

### 2) Fit polynomials of increasing degree

Suppose now that we didn't know the true relation between $Y$ and $X$ and that we were only given the dataset `df.train` generated above. 

In this case, we could assume that the relation between $Y$ and $X$ was a polynomial of degree $d$ summed with Gaussian noise, as in
$$
Y = \beta_0 + \sum_{k = 1}^d \beta_k X^k + \varepsilon~,
$$
and then our goal would be to decide which $d$ is the most appropriate and estimate the values for $\{\beta_0, \beta_1,\dots, \beta_d\}$. 

We can use the tools from multiple linear regression to estimate the parameter, even though the relation between $Y$ and $X$ was not assumed as linear. We can do the following:

1. Create $d+1$ predictors $X_i$ such that $X_i = X^i$ with $i = 0, \dots, d+1$
2. Consider the Gaussian multiple linear regression model given by
$$
Y = \beta_0 + \sum_{i = 1}^d \beta_i X_i + \varepsilon
$$
3. Obtain $\{\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_{d}\}$ as usually done in multiple linear regression

Note, however, that we usually don't know which $d$ to use in advance, so we often are obliged to fit several models of increasing order and then check which one of them seem the most adequate. There are many ways of using `R` to estimate polynomials of increasing order for this dataset. 

**Option 1: The naive approach.**
We can choose a value for $d_\max$ and then write by hand the formulae for each model and store the results appropriately:

```{r}
d_max = 3
fm_0 = lm(y ~ 1, data=df.train)
fm_1 = lm(y ~ 1 + x, data=df.train)
fm_2 = lm(y ~ 1 + x + x^2, data=df.train)
fm_3 = lm(y ~ 1 + x + x^2 + x^3, data=df.train)
```

Although this option is perfectly fine, it is not scalable for larger values of $d_\max$.

**Option 2: The good approach.**
A better approach is to fix $d_\max$ and then generate a set of several polynomials of increasing degree automatically as in:
```{r}
# choose the maximum degree for the polynomes
d_max = 9
# create an array of strings where each coordinate corresponds to a polynome of increasing order
poly.formulae <- c("y~1", paste("y ~ poly(x,", 1:d_max, ")", sep = ""))
# turn the strings into actual formula for R
poly.formulae <- sapply(poly.formulae, as.formula)
# create an empty list to store the results of the fitted models
fitted.models <- list(length = length(poly.formulae))
# loop over the polynomials of increasing degree
for (i in 1:length(poly.formulae)) 
{
  # use lm with the formula given by the array
  fm_i <- lm(formula = poly.formulae[[i]], data=df.train)
  # store the model in the list
  fitted.models[[i]] <- fm_i
}
```

### 3) Plot the fitted polynomials

Once we fit the models on the training dataset, we may want to plot them and see how close their predictions are to the observed $Y$. Note, however, that we should generate new predictions on values for $X$ that allow us to actually plot the fitted polynomials.

```{r}
# scatter plot of the data points used to fit the models
plot(x, y)
# create a plot dataset with the values of X on which to apply the fitted models
df.plot <- data.frame(x = seq(min(x), max(x), length.out = 200))
# loop over the different fitted models
for (i in 1:length(poly.formulae)) 
{
  fm_i <- fitted.models[[i]]
  # predict the values Y that the model will give for the df.plot
  y_i = predict(fm_i, newdata = df.plot)
  # plot an interpolated curve over the predicted values for the fitted model
  lines(df.plot$x, y_i, lty = i)
}
```

### 4) Assessing the training error

Although plotting the curves for each polynomial all together in the same figure is nice, the figure above contains way too much information and it might be hard to make any conclusions as to which model is the best one. A different way of assessing the quality of the models would be to plot how the training error evolves with the degree of the polynomials.

```{r}
# calculate the mean squared error (MSE) for each of the fitted models
error.train <- sapply(X=fitted.models, 
                FUN=function(mdl){mean(residuals(mdl)^2)})
# plot the training error in log-scale
plot(0:d_max, error.train, type = "b", xlab = "polynomial degree", 
     ylab = "mean squared error", log = "y")
```
We see that increasing the polynomial degree will always reduce the training mean squared error (MSE) and this might give us the impression that the "*larger the model, the better*". However, the **training MSE is not the right metric to use** when assessing the quality of a fitted model, since it is not measuring how well it generalizes to other data points.

### 5) Comparing training and generalization errors

We have seen in CM3 that the correct way of assessing the quality of a model is by checking how well it behaves on **unseen data**, i.e. data that was not used to train the model itself. Since we are doing simulations and we know the true generating model for the data, we can simply generate a new dataset with a lot of data points and check the error of our fitted model on them. The average of these errors will converge to the generalization error and will not be very far from it if the size of the dataset is large.

```{r}
x.test = rnorm(20e3)
y.test = 7 * x.test^2 - 0.5 * x.test + rnorm(20e3)
error.test <- sapply(X=fitted.models,
                     FUN=function(mdl){mean((y.test - predict(mdl, data.frame(x = x.test)))^2)})
plot(0:9, error.train, type = "b", xlab = "polynomial degree",
     ylab = "mean squared error", log = "y", ylim = c(min(error.train), max(error.train)))
lines(0:9, error.test, lty = 2, col = "blue")
points(0:9, error.test, pch = 24, col = "blue")
```

We see now that the generalization error (in blue) increases for larger and larger models (i.e. polynomials with more and more degrees), so a wise choice for the degree of the polynomial to fit to our dataset would be between 2 and 4, since this is where the generalization MSE is the lowest.

