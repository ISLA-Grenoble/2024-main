---
# title: "Template Work/labsheet"
# author: "Christophe Dutang, Pedro Rodrigues"
documentclass: article
papersize: a4
geometry: top=1.5cm, bottom=2cm, left=1.5cm, right=1.5cm
fontsize: 11pt
output: 
  pdf_document:
    extra_dependencies: ["enumitem"]
    number_sections: true
    toc: false
    keep_tex: false
    includes:
      in_header: "TD4-preamble.tex"
      before_body: "TD4-header.tex"
      
---

<!-- see help at https://bookdown.org/yihui/rmarkdown-cookbook/latex-output.html -->

```{r setup, include=FALSE, message=FALSE}
#see full list >knitr::opts_chunk$get()
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
```

\section*{$\blacktriangleright$~Exercise 1 (credits to EPFL CS-433)}
Binary logistic regression assumes
\begin{itemize}
\item[(A)] Linear relationship between the input variables.
\item[(B)] Linear relationship between the observations.
\item[(C)] Linear relationship between the input variables and the inverse sigmoid of the probability of the event that the outcome $Y = 1$.
\item[(D)] Linear relationship between the input variables and the probability of the event that the outcome $Y=1$.
\end{itemize}

\section*{$\blacktriangleright$~Exercise 2 (credits to Berkeley CS-189)}
Say we have two 2-dimensional Gaussian distributions representing two different classes. Which of the following conditions will result in a linear decision boundary
\begin{itemize}
\item[(A)] Same mean for both classes.
\item[(B)] Different covariance matrix for each class.
\item[(C)] Same covariance matrix for both classes.
\item[(D)] None of the above.
\end{itemize}

\section*{$\blacktriangleright$~Exercise 3 (credits to Berkeley CS-189)}
Which of the following are true about two-class Gaussian discriminant analysis? Assume you have estimated the parameters $\hat{\boldsymbol{\mu}}_1, \hat{\boldsymbol{\Sigma}}_1, \hat{\pi}_1$ for class 1 and $\hat{\boldsymbol{\mu}}_0, \hat{\boldsymbol{\Sigma}}_0, \hat{\pi}_0$ for class 0.
\begin{itemize}
\item[(A)] If $\hat{\boldsymbol{\mu}}_1 = \hat{\boldsymbol{\mu}}_0$ and $\hat{\pi}_1 = \hat{\pi}_0$, then the LDA and QDA classifiers are identical.
\item[(B)] If $\hat{\boldsymbol{\Sigma}}_1 = \boldsymbol{I}$ and $\hat{\boldsymbol{\Sigma}}_0 = 5\boldsymbol{I}$, then the LDA and QDA classifiers are identical.
\item[(C)] If $\hat{\boldsymbol{\Sigma}}_1 = \hat{\boldsymbol{\Sigma}}_0$, $\hat{\pi}_0 = 1/6$ and $\hat{\pi}_0 = 5/6$, then the LDA and QDA classifiers are identical.
\item[(D)] None of the above.
\end{itemize}

\section*{$\blacktriangleright$~Exercise 4 (credits to Berkeley CS-189)}
You want to train a dog identifier with Gaussian discriminant analysis. Your classifier takes an image vector as its input and outputs 1 if it thinks it is a dog, and 0 otherwise. You use the CIFAR10 dataset, modified so all the classes that are not "dog" have the label 0. Your training set has 5k dog images and 45k non-dog ("other") images. Which of the following statements seem likely to be correct.
\begin{itemize}
\item[(A)] LDA has an advantage over QDA because the two classes have different numbers of training examples.
\item[(B)] QDA has an advantage over LDA because the two classes have different numbers of training examples.
\item[(C)] LDA has an advantage over QDA because the two classes are expected to have very different covariance matrices.
\item[(D)] QDA has an advantage over LDA because the two classes are expected to have very different covariance matrices.
\end{itemize}

\section*{$\blacktriangleright$~Exercise 5 (credits to Berkeley CS-189)}
How does the bias-variance decomposition of a ridge regression estimator compare with that of ordinary least squares regression?
\begin{itemize}
\item[(A)] Ridge has larger bias, larger variance.
\item[(B)] Ridge has larger bias, smaller variance.
\item[(C)] Ridge has smaller bias, larger variance.
\item[(D)] Ridge has smaller bias, smaller variance.
\end{itemize}

\section*{$\blacktriangleright$~Exercise 6 (credits to EPFL CS-433)}
Assume we are doing linear regression with mean-squared loss and L2-regularization on four one-dimensional data points. Our prediction model can be written as $f(x) = a x + b$ and the optimization problem can be written as
$$
a^\star, b^\star = \underset{a, b}{\text{argmin}}~\sum_{i = 1}^4 \Big(y_i - f(x_i)\Big)^2 + \lambda a^2
$$
Assume that our data points $(x_i, y_i)$ are $\{(-2, 1), (-1, 3), (0, 2), (3, 4)\}$. What is the optimal value for the bias, $b^\star$?
\begin{itemize}
 \renewcommand{\labelitemi}{\scriptsize$\square$}
\item[(A)] Depends on the value of $\lambda$.
\item[(B)] 3
\item[(C)] 2.5
\item[(D)] None of the above answers.
\end{itemize}

\section*{$\blacktriangleright$~Exercise 7 (credits to CMU 10-701)}
Decide whether each of the following sentences is true or false. You must give at least a one sentence explanation for each of your choices.
\begin{itemize}
\item[(1)] Decision trees are learned by minimizing information gain
\item[(2)] The coefficients $\alpha$ assigned to the classifiers assembled by AdaBoost are always non-negative
\item[(3)] Maximizing the likelihood of logistic regression model yields multiple local optimums
\item[(4)] No classifier can do better than a naive Bayes classifier if the distribution of the data is known
\item[(5)] In the discriminative approach to solving classification problems, we model the conditional probability of the labels given the observations
\item[(6)] Students from ENSIMAG and PHELMA are trying to solve the same logistic regression problem for a dataset. The PHELMA group claims that their initialization point will lead to a much better optimum than ENSIMAGâ€™s initialization point. PHELMA is correct.
\item[(7)] In AdaBoost we start with a Gaussian weight distribution over the training samples
\item[(8)] A 1-NN classifier has higher variance than a 3-NN classifier
\item[(9)] If we had infinite data and infinitely fast computers, kNN would be the only algorithm we would study in our course.
\item[(10)] The correspondence between logistic regression and Gaussian Naive Bayes (with identity class covariances) means that there is a one-to-one correspondence between the parameters of the two classifiers
\end{itemize}



