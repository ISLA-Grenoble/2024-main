---
# title: "Template Work/labsheet"
# author: "Christophe Dutang, Pedro Rodrigues"
documentclass: article
papersize: a4
geometry: top=1.5cm, bottom=2cm, left=1.5cm, right=1.5cm
fontsize: 11pt
output: 
  pdf_document:
    extra_dependencies: ["enumitem"]
    number_sections: true
    toc: false
    keep_tex: false
    includes:
      in_header: "TD3-preamble.tex"
      before_body: "TD3-header.tex"
      
---

<!-- see help at https://bookdown.org/yihui/rmarkdown-cookbook/latex-output.html -->

```{r setup, include=FALSE, message=FALSE}
#see full list >knitr::opts_chunk$get()
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
```

\section*{$\blacktriangleright$~Exercise 1 (credits to EPFL CS-433)}
We consider a classification problem on linearly separable data. Our dataset had an outlier -- a point that is very far from the other datapoints in distance. We trained the linear discriminant analysis (LDA), logistic regression and 1-nearest-neighbour classifiers on this dataset. We tested the trained models on a test set that comes from the same distribution as the training set, but doesnâ€™t have any outlier points. After that, we removed the outlier and retrained our models.

After retraining, which classifier will **not change** its decision boundary around the test points.
\begin{itemize}
\item[(A)] Logistic regression.
\item[(B)] 1-nearest-neighbors classifier.
\item[(C)] LDA.
\item[(D)] None of them.
\end{itemize}

\section*{$\blacktriangleright$~Exercise 2 (credits to EPFL CS-433)}
In principal component analysis, the left singular vectors $\mathbf{U}$ of a data matrix $\mathbf{X} \in \mathbf{R}^{N \times p}$ are used to create a new data matrix $\mathbf{X}' = \mathbf{U}^\top\mathbf{X}$, where $N$ is the number of data points and $p$ is the number of features. Which property always holds for the matrix $\mathbf{X}'$?
\begin{itemize}
 \renewcommand{\labelitemi}{\scriptsize$\square$}
\item[(A)] $\mathbf{X}'$ is a square matrix.
\item[(B)] The mean of any row $\mathbf{X}'_i$ is 0
\item[(C)] $\mathbf{X}'$ has only positive values.
\item[(D)] For any two rows $i, j$ with $i\neq j$ from $\mathbf{X}'$, the dot product between the rows $\mathbf{X}'_i$ and $\mathbf{X}'_j$ is 0.
\end{itemize}

\section*{$\blacktriangleright$~Exercise 3 (credits to EPFL CS-433)}
Consider the logistic regression loss $\mathcal{L}: \mathbf{R}^p \to \mathbf{R}$ for a binary classification task with data $(\mathbf{x}_i, y_i) \in \mathbf{R}^p \times \{0, 1\}$:
$$
\mathcal{L}(\boldsymbol{\beta}) = \dfrac{1}{N}\sum_{i = 1}^N \left(\log\big(1 + e^{\mathbf{x}_i^\top \boldsymbol{\beta}}\big) - y_i \mathbf{x}_i^\top \boldsymbol{\beta}\right)
$$
Which of the following is a gradient of the loss $\mathcal{L}$?
\begin{itemize}
 \renewcommand{\labelitemi}{\scriptsize$\square$}
\item[(A)] $\nabla \mathcal{L}(\boldsymbol{\beta}) = \dfrac{1}{N}\displaystyle\sum_{i = 1}^N \left(\mathbf{x}_i \dfrac{e^{\mathbf{x}_i^\top \boldsymbol{\beta}}}{1 + e^{\mathbf{x}_i^\top \boldsymbol{\beta}}} - y_i \mathbf{x}_i^\top \boldsymbol{\beta}\right)$
\item[(B)] $\nabla \mathcal{L}(\boldsymbol{\beta}) = \dfrac{1}{N}\displaystyle\sum_{i = 1}^N \mathbf{x}_i\left(y_i - \dfrac{e^{\mathbf{x}_i^\top \boldsymbol{\beta}}}{1 + e^{\mathbf{x}_i^\top \boldsymbol{\beta}}}\right)$
\item[(C)] $\nabla \mathcal{L}(\boldsymbol{\beta}) = \dfrac{1}{N}\displaystyle\sum_{i = 1}^N \mathbf{x}_i\left(\dfrac{1}{1 + e^{\mathbf{x}_i^\top \boldsymbol{\beta}}} - y_i\right)$
\item[(D)] None of the above.
\end{itemize}

\newpage
\section*{$\blacktriangleright$~Exercise 4}
In this exercise you will perform multiple linear regression on simulated data under different conditions. To ensure reproducibility on your results, set the seed of the random with \code{set.seed(0)}. 

\begin{enumerate}
\item[(a)] Simulate a dataset of size $N = 1000$ of the following generating model:
\begin{equation*}
\begin{array}{rcl}
X_{1,i} &=& \varepsilon_{1,i} \\[0.25em]
X_{2,i} &=& 3X_{1,i} + \varepsilon_{2,i} \\[0.25em]
Y_{i} &=& X_{2,i} + X_{1,i} + 2 + \varepsilon_{3,i}
\end{array}
\end{equation*}
where $i \in \{1,\dots, N\}$ and the $\varepsilon_{ij}$ are independent $\mathcal{N}(0,1)$ random variables. For a given $i$, what is the distribution of $(X_{1,i}, X_{2,i})$? Plot the clouds of points of the simulated values of $(X_{1,i}, X_{2,i})_{i=1, \dots, n}$. What is its shape? Can you write an analytical formula for it?

\item[(b)] Let us consider the following two regression models:
\begin{equation*}
\begin{array}{rrcl}
\text{Model A:} & Y_i & = & \alpha_1 X_{1,i} + \alpha_0 + \tilde{\varepsilon}_{A,i} \\[0.5em]
\text{Model B:} & Y_i & = & \beta_2 X_{2,i} + \beta_0 + \tilde{\varepsilon}_{B,i}
\end{array}
\end{equation*}
where $\tilde{\varepsilon}_{A,i} \sim \mathcal{N}(0, \sigma_A^2)$ and $\tilde{\varepsilon}_{B,i} \sim \mathcal{N}(0, \sigma_B^2)$. What should be the values of $\hat{\alpha}_0, \hat{\alpha}_1, \hat{\sigma}_A^2, \hat{\beta}_0, \hat{\beta}_2, \hat{\sigma}_B^2$ when $N \to \infty$? Consider $N = 1000$ and check whether the estimates of the parameters are close to the true values that you've calculated. Now do \code{set.seed(3)} and simulate again a dataset $X_{1,i}, X_{2,i}, Y_i$ for $n = 10$. Estimate the parameters. What happens?

\item[(c)] Let us now consider the full model
$$
Y_i = \gamma_2 X_{2, i} + \gamma_1 X_{1, i} + \gamma_0 + \varepsilon_i
$$
where $i \in \{1,\dots, n\}$ and the $\varepsilon_i$ are independent $\mathcal{N}(0, \sigma^2)$ random variables. For the previously simulated data with $n = 10$, estimate $\hat{\gamma}_0, \hat{\gamma}_1, \hat{\gamma}_2, \hat{\sigma}^2$ and compare them with the parameters obtained in item (b). What can you say about the effects of $X_1$ and $X_2$ on $Y$? And about their correlation?
\end{enumerate}

\section*{$\blacktriangleright$~Exercise 5}
We consider the dataset \code{cars04}, which describes several properties of
different car models in the market in 2004. Each observation (i.e. car) is described by 11 features (i.e. properties) listed in Table 1.

\begin{table}[!hbt]
\begin{tabular}{ll}
Variable & Meaning\\
\hline
\code{Retail}  & Builder recommended price(US\$)\\
\code{Dealer}  & Seller price (US\$)\\
\code{Engine}  & Motor capacity  (liters)\\
\code{Cylinders}  & Number of cylinders in the motor\\
\code{Horsepower}  &Engine power\\
\code{CityMPG}  & Consumption in city (Miles or gallon; proportional to km/liter)\\
\code{HighwayMPG}  & Consumption on roadway  (Miles or gallon)\\
\code{Weight}  & Weight (pounds)\\
\code{Wheelbase}  & Distance between front and rear wheels (inches)\\
\code{Length}  & Length (inches)\\
\code{Width}   & Width  (inches)\\
\hline
\end{tabular}
\caption{Variable list for \code{cars04}}
\label{tab:cars04:varlist}
\end{table}


```{r, echo=FALSE}
## Load the "cars04" dataset
cars04 <- read.csv("cars-fixed04.csv")[,8:18]
```

The aim of this exercise is to summarize and to interpret the data \code{cars04} using PCA. Using `R` we run the following instruction:
```{r}
cars04.pca <- prcomp(cars04, scale=TRUE)
summary(cars04.pca)
```
\begin{enumerate}
\item[(a)] What is the effect of the argument \code{scale=TRUE} in the result of the PCA?
\item[(b)] Are the first two principal components enough to summarize most of the information (i.e. variance) of the dataset? Justify in terms of the proportion of the total variance that they represent.
\end{enumerate} 
Principal components are linear combinations of the 11 variables. 
The coefficients of the first 2 principal components on the 11 feature are
```{r}
cars04.pca$rotation[,1:2]
```
\begin{enumerate}[resume]
\item[(c)] What would be a good interpretation for these new variables in terms of the initial features of the dataset?
\end{enumerate} 

Figure \ref{fig:cars04:PCA} shows the projection of the dataset on its first two principal components.

\begin{enumerate}[resume]
\item[(d)] Interpret each quadrant of the figure.
\item[(e)] Can you describe which kind of car Audi RS 6, Ford Expedition 4.6 XLT and Nissan Sentra 1.8 are?
\end{enumerate} 

```{r, echo=FALSE, eval=FALSE}
lambda = cars04.pca$sdev^2
p = dim(cars04)[2]

x = (-100:100)/100
y = sqrt(1-x^2)
P = cars04.pca$rotation
for(i in 1:p) P[,i] = P[,i] * sqrt(lambda[i])

plot(P[,1],P[,2], xlab="Axis 1", ylab = "Axis 2", xlim=c(-1,1), ylim=c(-1,1))
abline(h=0, v=0)
lines(x, y)
lines(x, -y)
text(P[, 1], P[, 2]-0.05, names(cars04))
```


\begin{figure}[htp]
  \centering
  \begin{subfigure}[b]{0.47\textwidth}
```{r, echo=FALSE, fig.width=5, fig.height=5}
x <- (-100:100)/100
y <- sqrt(1-x^2)
lambda = cars04.pca$sdev^2
P = cars04.pca$rotation
for(i in 1:NCOL(P)) 
  P[,i] <- P[,i] * sqrt(lambda[i])

plot(P[,1], P[,2], xlab="Axis 1", ylab = "Axis 2", xlim=c(-1,1), ylim=c(-1,1))
abline(h=0, v=0)
lines(x, y)
lines(x, -y)
text(P[, 1]+0.1*rep(c(1,-1), length=NCOL(cars04)), 
     P[, 2]-0.1*rep(c(1,-1), length=NCOL(cars04)), names(cars04))

```     
     \caption{Variable space}
     \label{fig:cars04:PCA:variable}
  \end{subfigure}
  \hspace{5pt}
  \begin{subfigure}[b]{0.47\textwidth}
```{r, echo=FALSE, fig.width=5, fig.height=5}
somerows <- c("Audi RS 6", "Ford Expedition 4.6 XLT", "Nissan Sentra 1.8")
samplrows <- sample(setdiff(rownames(cars04), somerows), 25)
plot(cars04.pca$x[, 1], cars04.pca$x[, 2], xlab="PC1", ylab="PC2", type="n", xlim=c(-8,8), ylim=c(-4,4))
abline(h=0, v=0)
text(cars04.pca$x[samplrows, 1], cars04.pca$x[samplrows, 2], samplrows, cex=0.6, col="grey40")
text(cars04.pca$x[somerows, 1], cars04.pca$x[somerows, 2], somerows, cex=1.05)
```
    \caption{Individual space}
     \label{fig:cars04:PCA:individual}
  \end{subfigure}
\caption{Principal component representation in the first plane of the variable and of the sample spaces.}
  \label{fig:cars04:PCA}
\end{figure}

