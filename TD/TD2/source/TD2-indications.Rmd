---
# title: "Template Work/labsheet"
# author: "Christophe Dutang, Pedro Rodrigues"
documentclass: article
papersize: a4
geometry: top=1.5cm, bottom=2cm, left=1.5cm, right=1.5cm
fontsize: 11pt
output: 
  pdf_document:
    extra_dependencies: ["enumitem"]
    number_sections: true
    toc: false
    keep_tex: false
    includes:
      in_header: "TD1-preamble.tex"
      before_body: "TD1-header.tex"
      
---

<!-- see help at https://bookdown.org/yihui/rmarkdown-cookbook/latex-output.html -->

```{r setup, include=FALSE, message=FALSE}
#see full list >knitr::opts_chunk$get()
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
```

\section*{$\blacktriangleright$~Exercise 1}

Let $U$ and $V$ be two independent random variables with uniform distribution
over $[0,1]$. 

Let $X = U+V$ and $Y=U-V$. 

\begin{enumerate}
\item[(a)] Compute the expectation and covariance matrix of $Z = \left( 
\begin{array}{cc} X & Y\end{array} \right)^T$. %$Z = (X, Y)^T$.
\item[(b)] Prove that $X$ and $Y$ are uncorrelated but not independent.
\end{enumerate}

\vspace{1em}
\textcolor{blue}
{\textbf{Indications}.
Recall that the variance of the uniform distribution is 
$1/12$. Compute $\mathbb{E}[X]$, $\mathbb{E}[Y]$, $\text{Var}[X]$, $\text{Var}[
Y]$ and $\text{Cov}(X,Y)$. The solution is $\mathbb{E}[Z] = \left( \begin{array}
{c} 1 \\ 0\end{array} \right)$ and  $\Sigma_Z= \left( \begin{array}{cc} 1/6 &0\\ 0 
& 1/6 \end{array} \right)$. $X$ and $Y$ are uncorrelated but not independent. 
This is possible because the vector is not Gaussian.}

<!---------------------------------------------------------------------------->

\section*{$\blacktriangleright$~Exercise 2}

Let $X$ be a random vector in $\mathbb{R}^n$ and $A$ be a deterministic $m 
\times n$ matrix. 

\begin{enumerate}
\item[(a)] Prove that $$K_X = \mathbb{E} \left[ (X - \mathbb{E}[X])~(X - 
\mathbb{E}[X])^T \right] = \mathbb{E}[X X^T] - \mathbb{E}[X]~\mathbb{E}[X]^T~.$$
\item[(b)] Prove that $$K_{AX} = A~K_X~A^T~.$$
\item[(c)] Use the result obtained in (b) to derive again the results of 
Exercise 1.
\end{enumerate}

\vspace{1em}
\textcolor{blue}
{\textbf{Indications}.
In exercises (a) and (b) we just do a manipulation of vectors and
matrices. In exercise (c) write $$Z = \left( \begin{array}{cc} 1 &1\\ 1 &-1\end{array}
\right) \, \left( \begin{array}{c} U \\ V\end{array} \right)$$}

<!---------------------------------------------------------------------------->

\section*{$\blacktriangleright$~Exercise 3}

Let $Z = \left( \begin{array}{cc} X & Y\end{array} \right)^T$ be a Gaussian
vector with mean $\mu = \left( \begin{array}{cc} 1 & 2\end{array} \right)^T$ 
and covariance $\Sigma = \left( \begin{array}{cc} 1 &-1\\ -1 &2\end{array} 
\right)$.

\begin{enumerate}
\item[(a)] Compute the probability density function of $Z$.
\item[(b)] Using $$f_{Y|X=x}(y) = {\frac{f_{(X,Y)}(x,y)}{f_{X}(x)}}$$ compute
the distribution of $Y$ given $X=x$.
\item[(c)] What is the best prediction of $Y$ given $X=x$?
\end{enumerate}

\vspace{1em}
\textcolor{blue}
{\textbf{Indications.}
\begin{enumerate}
\item[(a)] Use the general formula of the density of a Gaussian vector.
$$f_{(X,Y)}(x,y)=\frac{1}{2\pi}
\exp\left[-\frac{1}{2} (2 x^2 + y^2 + 2xy - 8x - 6y +10)\right]$$
\item[(b)]$$f_{Y|X=x}(y) = {\frac{1}{\sqrt{2\pi}} \exp\left[-\frac{1}{2} \Big(y - 
(3-x)\Big)^2\right]} = {\cal N}(3-x,1)$$
\item[(c)]The best prediction of $Y$ given $X=x$ is $3-x$.
\end{enumerate}}

<!---------------------------------------------------------------------------->

\section*{$\blacktriangleright$~Exercise 4}

Let $\left( \begin{array}{cc} X & Y\end{array} \right)^T$ be a Gaussian vector in
$\mathbb{R}^2$. Let $Z=Y-\mathbb{E}[Y]-{\frac{\text{Cov}(X,Y)}{\text{Var}[X]}} \left(X - \mathbb{E}[X]\right)$.

\begin{enumerate}
\item[(a)] Compute $\mathbb{E}[Z]$ and $\text{Var}[Z]$.
\item[(b)] Prove that $X$ and $Z$ are independent.
\item[(c)] Derive the distribution of $Y$ given $X=x$.
\item[(d)] Use (c) to derive again the result of Exercise 3.
\end{enumerate}

\vspace{1em}
\textcolor{blue}
{\textbf{Indications.}
\begin{enumerate}
\item[(a)] $\mathbb{E}[Z] = 0$ and $\text{Var}[Z] = \text{Var}[Y] - {\frac{\text{Cov}(X,Y)^2}{\text{Var}[X]}}$.
\item[(b)] $\left( \begin{array}{cc} X & Z\end{array} \right)^T$ is a linear transform 
of $\left( \begin{array}{cc} X & Y\end{array} \right)^T$ so it is also a Gaussian
vector. $\text{Cov}(X, Z) = 0$, so $X$ and $Z$ are independent.
\item[(c)] The distribution of $Y$ given $X=x$ can be derived from that of $Z$ given
$X=x$ by a translation. The distribution of $Z$ given $X=x$ is the distribution
of $Z$. Therefore, the distribution of $Y$ given $X=x$ is the normal with
$$\mu = \mathbb{E}[Y] + {\frac{\text{Cov}(X,Y)}{\text{Var}[X]}} \left(x - \mathbb{E}[X]\right) 
\quad \text{and} \quad \sigma^2 = \text{Var}[Y] - {\frac{\text{Cov}(X,Y)^2}{\text{Var}[X]}}$$
Remember that for Gaussian vectors the best prediction of $Y$ given $X=x$ is affine.
\item[(d)] In the case of Exercise 3, the mean and variance are $3-x$ and $1$.
\end{enumerate}}

<!---------------------------------------------------------------------------->

\section*{$\blacktriangleright$~Exercise 5}
Consider the Gaussian simple linear regression model presented in class
$$
Y = \beta_0 + \beta_1 X_1 + \varepsilon \quad \text{with} \quad \varepsilon \sim \mathcal{N}(0, \sigma^2)
$$
The estimates for the parameters of the model, $\hat{\beta}_0$ and $\hat{\beta}_1$, are obtained $N$ paired samples $(X_i, Y_i)$.
\begin{enumerate}
\item[(a)] Show that the estimated parameters are unbiased.
\item[(b)] Show that $$\text{Var}(\hat{\beta}_1) = \dfrac{\sigma^2}{N}~\dfrac{1}{s_X^2} \quad \text{and} \quad \text{Var}(\hat{\beta_0}) = \dfrac{\sigma^2}{N}\left(1+\dfrac{\bar{X}^2}{s_X^2}\right)$$
where $\bar{X} = \dfrac{1}{N}\displaystyle\sum_{i = 1}^N X_i$ and $s_X^2 = \dfrac{1}{N}\displaystyle\sum_{i = 1}^N (X_i - \bar{X})^2$.
\end{enumerate}
Using the estimated parameters, we can predict that for a given arbitrary value of $X$, say $x$ (sometimes called the operation point) we that on average $Y$ will be $$\hat{m}(x) = \hat{\beta}_0 + \hat{\beta}_1 x$$ 
\begin{enumerate}
\item[(c)] Show that 
$$\mathbb{E}\Big[\hat{m}(x)\Big] = \beta_0 + \beta_1 x$$
\item[(d)] Show that $$\text{Var}\Big(\hat{m}(x)\Big) = \dfrac{\sigma^2}{N}\left(1 + \dfrac{(x - \bar{X})^2}{s_X^2}\right)$$Describe how the variance changes for different choices of the operation point.
\end{enumerate}