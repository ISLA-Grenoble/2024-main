---
# title: "Template Work/labsheet"
# author: "Christophe Dutang, Pedro Rodrigues"
documentclass: article
papersize: a4
geometry: top=1.5cm, bottom=2cm, left=1.5cm, right=1.5cm
fontsize: 11pt
output: 
  pdf_document:
    extra_dependencies: ["enumitem"]
    number_sections: true
    toc: false
    keep_tex: false
    includes:
      in_header: "TD2-preamble.tex"
      before_body: "TD2-header.tex"
      
---

<!-- see help at https://bookdown.org/yihui/rmarkdown-cookbook/latex-output.html -->

```{r setup, include=FALSE, message=FALSE}
#see full list >knitr::opts_chunk$get()
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
```

\section*{$\blacktriangleright$~Exercise 1}
Consider dataset $\mathcal{D} = \Big\{(\mathbf{x}_i, y_i)\Big\}_{i = 1}^{i = N}$ with $\mathbf{x}_i \in \mathbb{R}^p$ and $y_i \in \mathbb{R}$.

\begin{enumerate}
\item[(a)] Show that
\begin{equation*}
\dfrac{1}{N}\sum_{i = 1}^N \|\mathbf{x}_i - \bar{\mathbf{x}}\|^2 = \text{tr}(\mathbf{\Sigma})
\end{equation*}
where $\bar{\mathbf{x}}_i$ is the average of the samples of the dataset and $\mathbf{\Sigma}$ is their sample covariance matrix.
\item[(b)] Show that if the samples are standardized (i.e. they have zero mean and unit standard deviation) then
\begin{equation*}
\dfrac{1}{N}\sum_{i = 1}^N \|\mathbf{x}_i\|^2 = p
\end{equation*}
\item[(c)] Solve the matrix-valued optimization problem below (with $q < p$)
\begin{equation*}
\underset{\mathbf{W}^\top \mathbf{W} = \mathbf{I}_q}{\text{minimize}}~ \text{tr}(\mathbf{W}^\top\mathbf{\Sigma}\mathbf{W})
\end{equation*}
\end{enumerate}

<!---------------------------------------------------------------------------->

\section*{$\blacktriangleright$~Exercise 2}

Define $\mathbf{X}$ as a $N \times p$ data matrix with $\mathbf{x}_i$ vectors on its rows. 

Define also the vector $\mathbf{y} \in \mathbb{R}^N$ containing the observations $y_i$.

Suppose that both the features and the observations have been re-centered so to have zero mean.

\begin{enumerate}
\item[(a)] Show that the intercept of a multiple linear using this dataset will necessarily be zero.
\item[(b)] Use the singular value decomposition (SVD) of $\mathbf{X}$ to write an expression for the parameters $\boldsymbol{\hat{\beta}}$ of the multiple linear regression model 
\begin{equation*}
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}
\end{equation*}
\end{enumerate}
Consider now that we project the data matrix onto a subspace spanned by the $q$-top principal components of the data matrix $\mathbf{X}$ with $q < p$. Call this new data matrix $\mathbf{Z}$.
\begin{enumerate}
\item[(c)] Use the SVD of $\mathbf{Z}$ to write an expression for the parameters $\boldsymbol{\hat{\gamma}}$ of the multiple linear regression model 
\begin{equation*}
\mathbf{y} = \mathbf{Z}\boldsymbol{\gamma} + \boldsymbol{\varepsilon}
\end{equation*}
\item[(d)] Compare and interpret the expressions obtained in exercises (b) and (c).
\end{enumerate}

<!---------------------------------------------------------------------------->

\section*{$\blacktriangleright$~Exercise 3}

We consider the dataset \code{cars04}, which describes several properties of
different car models in the market in 2004. Each observation (i.e. car) is described by 11 features (i.e. properties) listed in Table 1.

\begin{table}[!hbt]
\begin{tabular}{ll}
Variable & Meaning\\
\hline
\code{Retail}  & Builder recommended price(US\$)\\
\code{Dealer}  & Seller price (US\$)\\
\code{Engine}  & Motor capacity  (liters)\\
\code{Cylinders}  & Number of cylinders in the motor\\
\code{Horsepower}  &Engine power\\
\code{CityMPG}  & Consumption in city (Miles or gallon; proportional to km/liter)\\
\code{HighwayMPG}  & Consumption on roadway  (Miles or gallon)\\
\code{Weight}  & Weight (pounds)\\
\code{Wheelbase}  & Distance between front and rear wheels (inches)\\
\code{Length}  & Length (inches)\\
\code{Width}   & Width  (inches)\\
\hline
\end{tabular}
\caption{Variable list for \code{cars04}}
\label{tab:cars04:varlist}
\end{table}


```{r, echo=FALSE}
## Load the "cars04" dataset
cars04 <- read.csv("cars-fixed04.csv")[,8:18]
```

The aim of this exercise is to summarize and to interpret the data \code{cars04} using PCA. Using `R` we run the following instruction:
```{r}
cars04.pca <- prcomp(cars04, scale=TRUE)
summary(cars04.pca)
```
\begin{enumerate}
\item[(a)] What is the effect of the argument \code{scale=TRUE} in the result of the PCA?
\item[(b)] Are the first two principal components enough to summarize most of the information (i.e. variance) of the dataset? Justify in terms of the proportion of the total variance that they represent.
\end{enumerate} 
Principal components are linear combinations of the 11 variables. 
The coefficients of the first 2 principal components on the 11 feature are
```{r}
cars04.pca$rotation[,1:2]
```
\begin{enumerate}[resume]
\item[(c)] What would be a good interpretation for these new variables in terms of the initial features of the dataset?
\end{enumerate} 

Figure \ref{fig:cars04:PCA} shows the projection the dataset on its first two principal components.

\begin{enumerate}[resume]
\item[(d)] Interpret each quadrant of the Figure.
\item[(e)] Can you describe which kind of car Audi RS 6, Ford Expedition 4.6 XLT and Nissan Sentra 1.8 are?
\end{enumerate} 

```{r, echo=FALSE, eval=FALSE}
lambda = cars04.pca$sdev^2
p = dim(cars04)[2]

x = (-100:100)/100
y = sqrt(1-x^2)
P = cars04.pca$rotation
for(i in 1:p) P[,i] = P[,i] * sqrt(lambda[i])

plot(P[,1],P[,2], xlab="Axis 1", ylab = "Axis 2", xlim=c(-1,1), ylim=c(-1,1))
abline(h=0, v=0)
lines(x, y)
lines(x, -y)
text(P[, 1], P[, 2]-0.05, names(cars04))
```


\begin{figure}[htp]
  \centering
  \begin{subfigure}[b]{0.47\textwidth}
```{r, echo=FALSE, fig.width=5, fig.height=5}
x <- (-100:100)/100
y <- sqrt(1-x^2)
lambda = cars04.pca$sdev^2
P = cars04.pca$rotation
for(i in 1:NCOL(P)) 
  P[,i] <- P[,i] * sqrt(lambda[i])

plot(P[,1], P[,2], xlab="Axis 1", ylab = "Axis 2", xlim=c(-1,1), ylim=c(-1,1))
abline(h=0, v=0)
lines(x, y)
lines(x, -y)
text(P[, 1]+0.1*rep(c(1,-1), length=NCOL(cars04)), 
     P[, 2]-0.1*rep(c(1,-1), length=NCOL(cars04)), names(cars04))

```     
     \caption{Variable space}
     \label{fig:cars04:PCA:variable}
  \end{subfigure}
  \hspace{5pt}
  \begin{subfigure}[b]{0.47\textwidth}
```{r, echo=FALSE, fig.width=5, fig.height=5}
somerows <- c("Audi RS 6", "Ford Expedition 4.6 XLT", "Nissan Sentra 1.8")
samplrows <- sample(setdiff(rownames(cars04), somerows), 25)
plot(cars04.pca$x[, 1], cars04.pca$x[, 2], xlab="PC1", ylab="PC2", type="n", xlim=c(-8,8), ylim=c(-4,4))
abline(h=0, v=0)
text(cars04.pca$x[samplrows, 1], cars04.pca$x[samplrows, 2], samplrows, cex=0.6, col="grey40")
text(cars04.pca$x[somerows, 1], cars04.pca$x[somerows, 2], somerows, cex=1.05)
```
    \caption{Individual space}
     \label{fig:cars04:PCA:individual}
  \end{subfigure}
\caption{Principal component representation in the first plane of the variable and of the sample spaces.}
  \label{fig:cars04:PCA}
\end{figure}
