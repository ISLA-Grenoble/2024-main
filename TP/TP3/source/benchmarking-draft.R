# Load required packages
library("MASS")
library("e1071")
library("class")

# Set seed for reproducibility
set.seed(42)

# Define function for generating data in Scenario 1
# (eps controls how hard the clf problem is)
generate_data_scenario_1 <- function(nsamples, eps)
{
  ndim = 2
  m0 = c(0, 0)
  s0 = rbind(c(1, 0), c(0, 2))
  z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
  x0 = (z + m0) %*% chol(s0)
  m1 = eps * c(1, 1) / sqrt(2)
  s1 = s0
  z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
  x1 = (z + m1) %*% chol(s1)
  X = rbind(x0, x1)
  y = c(rep(0, nsamples), rep(1, nsamples))
  df = data.frame(cbind(X, y))
  colnames(df) <- c('X1', 'X2', 'y')  
  return(df)
}

# Define function for generating data in Scenario 2
# (eps controls how hard the clf problem is)
generate_data_scenario_2 <- function(nsamples, eps)
{
  rho = -0.5
  ndim = 2
  m0 = c(0, 0)
  s0 = rbind(c(1, sqrt(2)*rho), c(sqrt(2)*rho, 2))
  z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
  x0 = (z + m0) %*% chol(s0)
  m1 = eps * c(1, 1) / sqrt(2)
  s1 = rbind(c(1, sqrt(2)*rho), c(sqrt(2)*rho, 2))
  z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
  x1 = (z + m1) %*% chol(s1)
  X = rbind(x0, x1)
  y = c(rep(0, nsamples), rep(1, nsamples))
  df = data.frame(cbind(X, y))
  colnames(df) <- c('X1', 'X2', 'y')  
  return(df)
}

# Define function for generating data in Scenario 3
# (eps controls how hard the clf problem is)
generate_data_scenario_3 <- function(nsamples, eps)
{
  rho = -0.5
  df = 10
  ndim = 2
  m0 = c(0, 0)
  s0 = rbind(c(1, sqrt(2)*rho), c(sqrt(2)*rho, 2))
  z = matrix(rt(nsamples*ndim, df=df), nrow=nsamples, ncol=ndim)
  x0 = (z + m0) %*% chol(s0)
  m1 = eps * c(1, 1) / sqrt(2)
  s1 = rbind(c(1, sqrt(2)*rho), c(sqrt(2)*rho, 2))
  z = matrix(rt(nsamples*ndim, df=df), nrow=nsamples, ncol=ndim)
  x1 = (z + m1) %*% chol(s1)
  X = rbind(x0, x1)
  y = c(rep(0, nsamples), rep(1, nsamples))
  df = data.frame(cbind(X, y))
  colnames(df) <- c('X1', 'X2', 'y')  
  return(df)
}

# Define function for generating data in Scenario 4
# (eps controls how hard the clf problem is)
generate_data_scenario_4 <- function(nsamples, eps)
{
  rho_0 = +0.5
  rho_1 = -0.5
  ndim = 2
  m0 = c(0, 0)
  s0 = rbind(c(1, sqrt(2)*rho_0), c(sqrt(2)*rho_0, 2))
  z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
  x0 = (z + m0) %*% chol(s0)
  m1 = eps * c(1, 1) / sqrt(2)
  s1 = rbind(c(1, sqrt(2)*rho_1), c(sqrt(2)*rho_1, 2))
  z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
  x1 = (z + m1) %*% chol(s1)
  X = rbind(x0, x1)
  y = c(rep(0, nsamples), rep(1, nsamples))
  df = data.frame(cbind(X, y))
  colnames(df) <- c('X1', 'X2', 'y')  
  return(df)
}

# Define function for generating data in Scenario 5
# (eps controls how hard the clf problem is)
generate_data_scenario_5 <- function(nsamples, eps)
{
  ndim = 2
  m0 = c(0, 0)
  s0 = rbind(c(2, 0), c(0, 4))
  z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
  x0 = (z + m0) %*% chol(s0)
  m1 = eps * c(1, 1) / sqrt(2)
  s1 = rbind(c(5, 0), c(0, 2))
  z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
  x1 = (z + m1) %*% chol(s1)
  X = rbind(x0, x1)
  y = c(rep(0, nsamples), rep(1, nsamples))
  df = data.frame(cbind(X, y))
  colnames(df) <- c('X1', 'X2', 'y')  
  return(df)
}

# choose scenario
generate_data = generate_data_scenario_4

# Decide the difficulty of the classification problem
eps = 1.25

# # Plot the data from the choosen scenario 
# df_plot = generate_data(nsamples=1000, eps=eps)
# X = df_plot[,1:2]
# y = df_plot[,3]
# plot(X, col='white', xlab="X1", ylab="X2")
# points(X[y == 0,], col='#1f77b4', pch=16)
# points(X[y == 1,], col='#ff7f0e', pch=16)

# Size of the training dataset
ntrain = 20
# Size of the test dataset
ntest = 5000
# How many repetitions to consider
nrzt = 100

# Run the loop
pb = txtProgressBar(min = 0, max = nrzt, initial = 0) 
error_lda <- c()
error_lrg <- c()
error_nvb <- c()
error_qda <- c()
for (i in 1:nrzt){

  # print(i)
  setTxtProgressBar(pb,i)

  # Generate training and test data
  df_train = generate_data(nsamples=ntrain, eps=eps)
  df_test = generate_data(nsamples=ntest, eps=eps)

  # LDA classification
  ldaclf = lda(y ~ ., data=df_train)
  y_pred_lda = predict(ldaclf, newdata=df_test)
  error_lda_i = sum(y_pred_lda$class != df_test[,3]) / (2*ntest)
  error_lda = c(error_lda, error_lda_i)

  # Logistic regression
  logregclf <- glm(y ~ ., data=df_train, family=binomial)
  y_pred_logreg <- predict.glm(logregclf, newdata=df_test, type="response")
  error_lrg_i = sum((y_pred_logreg > 0.5) != df_test[,3])/(2*ntest)
  error_lrg = c(error_lrg, error_lrg_i)

  # Gaussian naive Bayes
  naivclf <- naiveBayes(y ~ ., data=df_train)
  y_pred_naive <- predict(naivclf, newdata=df_test, type="raw")
  error_nvb_i = sum((y_pred_naive[,1] > 0.5) == df_test[,3]) / (2*ntest)
  error_nvb = c(error_nvb, error_nvb_i)

  # QDA classification
  qdaclf = qda(y ~ ., data=df_train)
  y_pred_qda = predict(qdaclf, newdata=df_test)
  error_qda_i = sum(y_pred_qda$class != df_test[,3]) / (2*ntest)
  error_qda = c(error_qda, error_qda_i)
}
close(pb)

df = data.frame(cbind(error_lda, error_lrg, error_nvb, error_qda))
boxplot(df)
